si = library("signals.lib");
ba = library("basics.lib");
ma = library("maths.lib");
df = library("diff.lib");
an = library("analyzers.lib");

declare name "Faust Automatic Differentiation Library";
declare version "0.0.1";
declare author "Thomas Rushton; Advik Raj Basani";

//==========================Variables of Interest===============================
//
//==============================================================================

// vars(vars) = environment {
//     N = outputs(vars);
//     var(i) = ba.take(i,vars),pds(N,i)
//     with {
//         pds(N,i) = par(j,N,i-1==j);
//     };
// };

// Possible to use pattern matching here?...
var(meter, init) = -~_ : _,init : + <: attach(meter);
// var0(meter) = var(meter, 0.0);
// var(meter) = var((meter,0.0));
// var((meter,init)) = -~_ : _,init : + <: attach(meter);

vars(V) = environment {
    // Count the variables.
    N = outputs(V);
    // Retrieve a variable by index i.
    var(i) = take(i, V),pds(N, i)
    with {
        // Compute partial derivatives of
        // variable x_i.
        pds(N, i) = par(j, N, i-1==j);

        take(i, L) = _ <: L:sel with {
            sel = par(j, i-1, !),_,par(j, outputs(L)-i, !);
        };
    };
};

// TODO: properly randomise initial values.
weightsAndBiases(LAYERSPEC) = vars(
    par(i, outputs(LAYERSPEC) / 2,
        par(j, ba.take(2 * i + 1, LAYERSPEC),
            par(k, ba.take(2 * i + 2, LAYERSPEC),
                df.var(hbargraph("[0] w%3i%3j%3k", -10, 10), sin((i+1)*(j+1)*(k+1)))
            ),df.var(hbargraph("[1] b%3i%3j", -10, 10), sin((i+1)*(j+1)))
        )
    )
);

// A differentiable variable.
// Returns a dual signal containing the variable and a vector of its partial derivatives.
// [ xi, dxi/dx1, dxi/dx2, ... , dxi/dxN ]
// var(I,var,nvars) = var,par(i,nvars,i+1==I);
// TODO: replace with something that encapsulates gradient descent and metering, e.g.
// var(nvars,I,graph) = -~_ <: attach(graph),par(i,nvars,i+1==I);
// TODO: also normalisation?

// A friendly way of handling a differentiable recursion.
// ngrads is the number of inputs available for passing in gradients.
rec(F ~ G, ngrads) = (G,si.bus(n) : F) ~ si.bus(m)
with {
    n = inputs(F) - outputs(G);
    m = inputs(G) - ngrads;
};

backprop(target, estimate, lossFunction) = (
        // Route gradients to estimate; route input signals to target and estimate.
        routes.crossNM(nvars, nInputs)
        : vgroup("Target", target),vgroup("Estimate", estimate)
        // Duplicate target and estimate output.
        : routes.splitCrossDown(2+nvars, 2)
        : vgroup("[1]Loss/gradient", lossFunction,_,_)
        // Feed the gradients back.
        ) ~ (!,si.bus(nvars))
        // Block the gradients, post-recursion.
    : _,si.block(nvars),_,_
with {
    nvars = inputs(estimate),inputs(target) : -;
    nInputs = inputs(target),2 : *;
};

paramOpt(target, estimate, lossFunction) = vgroup("[0]Target", target),_ : backprop
with {
    backprop(y) = (routes.crossNM(nvars, nInputs)
        : vgroup("[1]Estimate", estimate)
        : L
    ) ~ (!,!,si.bus(nvars))
    : y,_,_,si.block(nvars)
    with {
        L(yHat) = yHat,vgroup("[2]Loss & Gradients", lossFunction(y,yHat));
    };

    nvars = inputs(estimate),inputs(target) : -;
    nInputs = inputs(target);
};

//========================Differentiable environment============================
//
//==============================================================================

env(vars) = environment {
    // Forward mode automatic differentiation.

    // Automatically differentiate the circuit C in forward mode.
    // Make all inputs differentiable.
    fwdADIn(C) = par(i, inputs(C), input) : fwdAD(C);

    // Automatically differentiate the circuit C in forward mode.
    // Make just the first N inputs differentiable; treat the rest as
    // differentiable variables.
    fwdADInN(C, N) = par(i, N, input),par(j, inputs(C) - N, diff(_)) : fwdAD(C);

    // Automatically differentiate the circuit C in forward mode.
    // Limitations:
    // - It isn't possible to pattern-match lambda abstractions, so these cannot
    //   be differentiated.
    // - UI elements can only be pattern-matched on _all_ of their parameters,
    //   so, for practical purposes, these aren't differentiable.
    // - Similarly, the `route` primitive cannot be considered differentiable.
    fwdAD(C) = ad(C)
    with {
        ad(A ~ B) = ad(A) ~ ad(B);
        // This merge composition derivative makes explicit use of the sum iterator.
        // ad(A :> B) = ad(A : routes.interleave(inputs(B), k) : par(p, inputs(B), sum(q, k, _)) : B)
        // This merge composition derivative uses merge itself to produce the derivative.
        ad(A :> B) = ad(A : routes.interleave(inputs(B), k))
            : par(p, inputs(B), par(q, (1+vars.N)*k, _) :> par(r, 1+vars.N, _))
            : ad(B)
        with {
            // k taken from /Syntactical and Semantical Aspects of Faust/ https://hal.science/hal-02159011
            k = outputs(A) / inputs(B);
        };
        ad(A <: B) = ad(A) <: ad(B);
        ad((A,B)) = ad(A),ad(B);
        ad(A : B) = ad(A) : ad(B);
        ad(A) = diff(A);
    };

    //=====================Differentiable primitives============================
    //
    //==========================================================================

    // Differentiable identity function.
    // Returns nvars+1 parallel identity functions.
    diff(_) = par(i,vars.N+1,_);

    // Differentiable cut primitive.
    // Blocks everything. Useful for differentiable routing.
    // <u, u'> = <!, !>
    diff(!) = par(i, vars.N+1, !);

    // Differentiable addition.
    // Takes two dual numbers as input;
    // returns the sum and its partial derivatives.
    // <u, u'> + <v, v'> = <u+v, u'+v'>
    diff(+) = diffadd
    with{
        diffadd = routes.interleave(vars.N + 1, 2)
            : +,par(i, vars.N, +);

        diffadd2 = binary(+, (_,!,!,_ : +));
    };

    // Differentiable subtraction.
    // <u, u'> - <v, v'> = <u-v, u'-v'>
    diff(-) = diffsub
    with {
        diffsub = routes.interleave(vars.N + 1, 2)
            : -,par(i, vars.N, -);

        diffsub2 = binary(-, (_,!,!,_ : -));
    };

    // Differentiable multiplication.
    // Takes two dual numbers as input;
    // returns the product and its partial derivatives.
    // <u, u'> * <v, v'> = <u*v, u'*v + v'*u>
    diff(*) = diffmul with { diffmul = binary(*, (*,* : +)); };

    // Differentiable division.
    // Takes two dual numbers as input;
    // returns the quotient and its partial derivatives.
    // <u, u'> / <v, v'> = <u/v, (u'*v - v'*u) / v^2>
    diff(/) = diffdiv
    with {
        diffdiv = binary(/, (_,(_ <: _,_),_,_
                : _,_,routes.crossNM(1, 2)
                : *,*,(_ <: *)
                : -,_
                : / //safediv
        ));
    };

    // Differentiable exponentiation
    // Takes two dual signals as input;
    // Returns the first raised to the power of the second, and partial derivatives
    // of the exponentiation.
    // <u, u'> ^ <v, v'> = <u^v, u^{v-1}*(v*u' + ln(u)*u*v')>
    //
    // NB. due to the presence of the natural log, may require that a differentiable
    // variable never equals exactly zero, e.g.
    //   var = -~_,1e-10 : + ...
    diff(^) = diffpow with { diffpow = binary(^, (
        _,(_ <: _,-(1)),(_ <: _,*(log)),_
        : *,(routes.cross(2) : ^),*
        : routes.cross(2),_
        : _,+
        : *
    )); };

    // Differentiable modulo function
    // Takes a dual number as input...

    // Differentiable sine function
    // Takes a dual number as input;
    // Returns the sine of the input, and its partial derivatives.
    // sin(<u, u'>) = <sin(u), u'*cos(u)>
    diff(sin) = diffsin with { diffsin = unary(sin, (_,cos : *)); };

    // Differentiable cosine function
    // Takes a dual number as input;
    // Returns the cosine of the input, and its derivative.
    // cos(<u, u'>) = <cos(u), -u'*sin(u)>
    diff(cos) = diffcos with { diffcos = unary(cos, (*(-1),sin : *)); };

    // tan(<u, u'>) = <tan(u), u'/cos^2(u)>
    diff(tan) = difftan with { difftan = unary(tan, (_,(cos <: *) : /)); };

    // Differentiable asine function
    // Takes a dual number as input;
    // Returns the asine of the input, and its derivative.
    // asin(<u, u'>) = <asin(u), u'/sqrt(1 - u^2)>
    diff(asin) = diffasin with { diffasin = unary(asin, (_,sqrt(1 - (_ <: *)) : /)); };

    // Differentiable acosine function
    // Takes a dual number as input;
    // Returns the acosine of the input, and its derivative.
    // acos(<u, u'>) = <acos(u), -u'/sqrt(1 - u^2)>
    diff(acos) = diffacos with { diffacos = unary(acos, (*(-1),sqrt(1 - (_ <: *)) : /)); };

    // Differentiable atan function
    // Takes a dual number as input;
    // Returns the atan of the input, and its derivative.
    // atan(<u, u'>) = <atan(u), u'/(1 + u^2)>
    diff(atan) = diffatan with { diffatan = unary(atan, (_,1 + (_ <: *) : /)); };

    // Differentiable atan2 function
    // Takes a dual number as input;
    // Returns the atan2 of the input, and its derivative.
    // atan2(<u, u'>, <v, v'>) = <atan(u, v), (u'v - v'u) / (u^2 + v^2)>
    diff(atan2) = diffatan2 with { diffatan2 = binary(atan2, (
        _,(_ <: _,_),(_ <: _,_),_
        : _,_,routes.crossNM(2, 2)
        : *,*,(_ <: *),(_ <: *)
        : -,+
        : / //safediv
    )); };

    // Differentiable exponential function
    // Takes a dual signal as input;
    // Returns the exponential function applied to the primal input, and its derivatives.
    // exp(<u, u'>) = <exp(u), u'*exp(u)>
    diff(exp) = diffexp with { diffexp = unary(exp, (_,exp : *)); };

    // Differentiable base-e logarithm
    // Takes a dual number as input;
    // Returns the log of the input, and its derivative.
    // log(<u, u'>) = <log(u), u'/u>
    // TODO: use safediv
    diff(log) = difflog with { difflog = unary(log, /); };

    // Differentiable base-10 logarithm
    // Takes a dual number as input;
    // Returns the log of the input, and its derivative.
    // log10(<u, u'>) = <log10(u), u'/(ln(10)*u)>
    // TODO: use safediv
    diff(log10) = difflog10 with { difflog10 = unary(log10, (_,*(log(10)) : /)); };

    // Differentiable square root
    // Takes a dual number as input;
    // Returns:
    //   Primal: the square root of the input.
    //   Tangents: partial derivatives of the square root operation.
    // sqrt(<u, u'>) = <sqrt(u), u'/(2*sqrt(u))>
    diff(sqrt) = diffsqrt with { diffsqrt = unary(sqrt, (_,2*sqrt : /)); };

    // Differentiable absolute value function
    // Takes a dual number as input;
    // Returns:
    //   Primal: the absolute value of the input.
    //   Tangents: partial derivatives of the absolute value of the input.
    // abs(<u, u'>) = <abs(u), u'*u/abs(u)>
    diff(abs) = diffabs with { diffabs = unary(abs, (_,(_ <: _,abs : positiveSafediv) : *)); };

    // Differentiable min function
    // Takes a dual number as input;
    // Returns the min of the input, and its derivative.
    // min(<u, u'>, <v, v'>) = <min(u, v), d>
    //                        d = { u' : (u < v)
    //                            { v' : (u >= v)
    diff(min) = diffmin
    with {
        diffmin = route(2*(vars.N+1), 2*(vars.N+2), (1,1), (vars.N+2, 2), (1,3), (vars.N+2, 4),
            par(i, vars.N, ((i+2), (i+5))), par(i, vars.N, (i+vars.N+3, i+vars.N+5)))
            : min, <, par(i, 2*vars.N, _)
            : _,
            route(2*(vars.N)+1, 3*vars.N,
                par(i, vars.N, (1, temp_1)
                    with {
                        temp_1 = 3*i+1;
                    }
                ),
                par(i, vars.N, (i+2, temp_2), (i+vars.N+2, temp_2 + 1)
                    with {
                        temp_2 = 3*i+2;
                    }
                )
            )
            : _,par(i, vars.N, select2);
    };

    // Differentiable max function
    // Takes a dual number as input;
    // Returns the max of the input, and its derivative.
    // max(<u, u'>, <v, v'>) = <max(u, v), d>
    //                        d = { u' : (u >= v)
    //                            { v' : (u < v)
    diff(max) = diffmax
    with {
        diffmax = route(2*(vars.N+1), 2*(vars.N+2), (1,1), (vars.N+2, 2), (1,3), (vars.N+2, 4),
            par(i, vars.N, ((i+2), (i+5))), par(i, vars.N, (i+vars.N+3, i+vars.N+5)))
            : max, >=, par(i, 2*vars.N, _)
            : _,
            route(2*(vars.N)+1, 3*vars.N,
                par(i, vars.N, (1, temp_1)
                    with {
                        temp_1 = 3*i+1;
                    }
                ),
                par(i, vars.N, (i+2, temp_2), (i+vars.N+2, temp_2 + 1)
                    with {
                        temp_2 = 3*i+2;
                    }
                )
            )
            : _,par(i, vars.N, select2);
    };

    // floor(<u, u'>) = <floor(u), u'>
    diff(floor) = difffloor with { difffloor = floor,par(i, vars.N, _); };

    // ceil(<u, u'>) = <ceil(u), u'>
    diff(ceil) = diffceil with { diffceil = ceil,par(i, vars.N, _); };

    // Differentiable one-sample delay
    // <u, u'>[n-1] = <u[n-1], u'[n-1]>
    diff(mem) = diffmem with { diffmem = mem,par(i, vars.N, mem); };

    // Differentiable int cast... derivative should be nonzero for sin(pi*u) == 0
    // int(<u, u'>) = <int(u), d>
    //                / u',  sin(pi*u) == 0, u increasing
    //            d = { -u', sin(pi*u) == 0, u decreasing
    //                \ 0,   otherwise
    // This isn't perfectly mathematically sound.
    // For algorithms where int cast is useful it's also unlikely that u will land
    // precisely on an integer value at time n.
    diff(int) = diffint1
    with{
        diffint = (
            _ <: (int <: _,_),_,_,_'
            // Check whether input is an integer,
            // and whether it's increasing or decreasing.
            : _,==,(<,1,-1 : select2)
            : _,*
        ),par(i,vars.N,_)
        : _,route(1+vars.N,vars.N*2,par(i,vars.N,(1,2*i+1),(i+2,2*i+2)))
        // And (chain rule) multiply by u'
        : _,par(i,vars.N,*);

        // Ignore the discontinuities?
        diffint1 = int,par(i, vars.N, _);

        // If u[n] > u[n-1], u has just stepped up by 1; if u[n] < u[n-1], u has stepped down by 1.
        diffint2 = (int <: _,(_,_',1,-1 : select2(<) <: par(i, vars.N, _))),par(i, vars.N, _)
            : _,routes.outerleave(vars.N, 2)
            : _,par(i, vars.N, *);
    };

    // Differentiable delay
    // Only really works for delays that increase because it's asymmetrical in time.
    // Still, it's useful as a component in differentiable algorithms that feature
    // fixed delays.
    //
    // <u, u'>[n-<v, v'>] = <u[n-v], u'[n-v] - v' * (u[n-v])'_n>
    //                    = u@v,u'@v,v',(u@v)'_n : _,_,* : _,-;
    diff(@) = diffdelay with { diffdelay = binary(@, (
        _,(_ <: _,_),_,_
        // : @,routes.cross(2),_
        // : _,(@ <: _,mem),_
        // : _,-,_
        // : _,*
        // : -
        : @ - (routes.cross(2) : @ <: _,mem : -)*_
    )); };

    //======================Differentiable utilities============================
    //
    //==========================================================================

    // A differentiable constant signal.
    // Returns the signal, plus its partial derivatives wrt all variables, which
    // are all zero.
    // <c, c'> = <c, 0>
    diff(c) = diffconst with{ diffconst = c,par(i, vars.N, 0); };

    // A differentiable (audio) input. Similar to a number.
    // Returns the input, plus its partial derivatives wrt all variables, which
    // are all zero.
    // <x, x'> = <x, 0>
    input = _,par(i, vars.N, 0);

    // f0 is a dual signal
    phasor(f0) = f0,diff(ma.SR)
        : diff(/)
        : rec(f~g,0),diff(2*ma.PI)
        : diff(*)
        with {
            f = diff(+) <: diff(_),diff(int) : diff(-);
            g = diff(_);
        };

    // f0 is a dual signal
    osc(f0) = phasor(f0) : diff(sin);

    // Differentiable sum iteration.
    // N: number of iterations
    sumall(N) = sumallImpl(N)
    with {
        sumallImpl(1) = diff(_);
        sumallImpl(2) = diff(+);
        sumallImpl(N) = seq(n,N-2,diff(+),par(m,N-n-2,diff(_))) : diff(+);
    };

    //======================== Neural Networks ================================
    //
    //=========================================================================

    neuron(NINPUTS, activationFunction) = Z : activationFunction
    with {
        WX = par(i, NINPUTS, vars.var(i+1),diff(_) : diff(*));
        b = vars.var(NINPUTS+1);

        Z = WX,b : sumall(NINPUTS + 1);
    };

    activations = environment {
        // Sigmoid activation function
        // y = 1 / (1 + e^{-z})
        sigmoid = fwdAD(*(-1) : 1 / (1 + exp));

        // ReLU
        // y = max(0, z) = (z + |z|) / 2
        relu = fwdAD(_ <: (_,abs) : + : /(2));

        // TODO: Leaky ReLU, GELU, tanh, etc.
    };

    // paramStartIndex: starting parameter index.
    layers(paramStartIndex) = environment {
        // Dense (or fully-connected) layer
        dense(nNeurons, wPerNeuron, activationFunction) = routeToNeurons
            : par(n, nNeurons, neuron(n, wPerNeuron, activationFunction))
        with {
            // For each neuron, route:
            // _ a weight gradient
            // _ a primal input
            // _ partial derivatives of the input
            // ...
            // _ a bias gradient
            routeToNeurons = route(nIn, nIn,
                // For each neuron...
                par(n, nNeurons,
                    // For each weight in that neuron...
                    par(w, wPerNeuron,
                        // Route the weight gradient
                        gradStart + w + n * (wPerNeuron + 1),
                        1 + n * neuronIns + w * (vars.N + 2),
                        // And route an input + PDs
                        par(i, vars.N + 1,
                            1 + n * (wPerNeuron * (vars.N + 1)) + w * (vars.N + 1) + i,
                            2 + n * neuronIns + w * (vars.N + 2) + i
                        )
                    ),
                    // Route the bias gradient
                    gradStart + (n + 1) * wPerNeuron + n,
                    (n + 1) * neuronIns
                ))
                with {
                    nIn = nNeurons * (wPerNeuron + wPerNeuron * (1 + vars.N) + 1);
                    neuronIns = wPerNeuron * (vars.N + 2) + 1;
                    gradStart = nNeurons * (wPerNeuron) * (vars.N + 1) + 1;
                }
                : par(n, nNeurons, par(w, wPerNeuron, _,diff(_)),_);
        };
    } with {
        // N: neuron index within layer.
        neuron(N, nWeights, activationFunction) = Z : activationFunction
        with {
            WX = par(i, nWeights, vars.var(startIdx + i),diff(_) : diff(*));
            b = vars.var(startIdx + nWeights);

            Z = WX,b : sumall(nWeights + 1);

            startIdx = paramStartIndex + N * (nWeights + 1) + 1;
        };
    };

    nn(LAYERSPEC, activationFunction) = routing : inputLayer,si.bus(vars.N)
        : seq(k, nLayers, layer(nLayers - k - 1)
        with {
            nNeurons = ba.take(2 * k + 1, LAYERSPEC);
            wPerNeuron = ba.take(2 * k + 2, LAYERSPEC);

            // Don't iteratively make the output layer, just pass signals through.
            // NB. layers are numbered in descending order.
            layer(0) = si.bus(nNeurons * wPerNeuron * (vars.N + 1) + nNeurons * (wPerNeuron + 1));
            layer(j) = (layers(idx).dense(nNeurons, wPerNeuron, activationFunction) <: si.bus(nextLayer.nInputs)),si.bus(nGrads)
            with {
                nextLayer = environment {
                    nNeurons = ba.take(2 * k + 3, LAYERSPEC);
                    wPerNeuron = ba.take(2 * k + 4, LAYERSPEC);
                    nInputs = nNeurons * wPerNeuron * (vars.N + 1);
                };

                // Number of gradients to pass on to succeeding layers.
                nGrads = vars.N - usedGrads;

                usedGrads = sum(g, k + 1, nNeurons * (wPerNeuron + 1)
                with {
                    nNeurons = ba.take(2 * g + 1, LAYERSPEC);
                    wPerNeuron = ba.take(2 * g + 2, LAYERSPEC);
                });

                idx = usedGrads - nNeurons * (wPerNeuron + 1);
            };
        })
        : outputLayer
        with {
            // Number of entries in the layer specification.
            nSpec = ba.count(LAYERSPEC);
            // Number of layers, i.e. number of nNeurons,wPerNeuron pairs in the layer specification.
            nLayers = nSpec / 2;
            // The number of inputs to the network is the product of the first pair of
            // entries in LAYERSPEC, i.e. the number of neurons times the number of weights
            // per neuron in the first layer.
            nInputs = ba.take(1, LAYERSPEC) * ba.take(2, LAYERSPEC);

            // Prior to the input layer, route inputs and gradients appropriately.
            routing = routes.crossNM(vars.N, nInputs);

            // Transform inputs into differentiable inputs.
            inputLayer = par(n, nInputs, input);

            // Build an output layer from the final two entries in LAYERSPEC.
            outputLayer = layers(idx).dense(nNeurons, wPerNeuron, activationFunction)
            with {
                nNeurons = ba.take(nSpec - 1, LAYERSPEC);
                wPerNeuron = ba.take(nSpec, LAYERSPEC);
                idx = vars.N - nNeurons * (wPerNeuron + 1);
            };
        };

    //===========================Learning Rate Schedulers=======================
    //
    //==========================================================================
    // initial_lr : learning_rate_scheduler(epochs)
    learning_rate_scheduler(epochs, delta) = _ : helper(epochs)~(_,_) : (!, _)
        with {
            helper(epochs) = ((_ <: _, _),_,_
                    : _, (_ == 0, _, _ : select2)
                    : ((_:+~1 <: attach(hbargraph("epochs", 0, 10))) <: _, _ % epochs == 0), (_ <: _, _ * exp(delta))
                    : _, (select2 <: attach(hbargraph("lr", 0.0001, 0.5))));
        };

    //===========================Optimizers=====================================
    //
    //==========================================================================
    // Stochastic gradient descent
    optimizers = environment {
        SGD(learningRate) = par(i,vars.N,_,learningRate : *);
        RMSProp(learningRate,rho) = par(i, vars.N, (_ <: _,(_ :((_ : ^(2)) : *(1 - rho)) : +~(_ : *(rho))))
                    : _,(sqrt : +(ma.EPSILON)) : / : *(learningRate));
        Adam(learningRate,beta1,beta2) = par(i, vars.N, (_ <: (_ : *(1 - beta1) : +~(_ : *(beta1)))
                    ,((_ : ^(2)) : *(1 - beta2) : +~(_ : *(beta2))))
                    : /(1 - beta1),/(1 - beta2)
                    : _,(sqrt : +(ma.EPSILON))
                    : / : *(learningRate));
    };

    //===========================Loss functions=================================
    //
    //==========================================================================

    losses = environment {
        L1 = lossFunction(abs);
        L2 = lossFunction(^(2));

        // spectral = environment {
        //     L1 =
        // };
    } with {
        lossFunction(function, windowSize, learningRate, y, yHat) = error,par(i,vars.N,_)
            : window
            : fwdAD(function)
            : meter
            : _,scaleGrads
        with {
            window = par(i, vars.N+1, ba.slidingMean(windowSize));
            error = yHat,y : -;
            meter = (_ <: attach(hbargraph("[100]loss",0.,.05))),
                par(i, vars.N, _ <: attach(hbargraph("gradient%3i",-.5,.5)));
            scaleGrads = par(i, vars.N, _,learningRate : *);
        };
    };

    // Stochastic gradient descent with time-domain L2 norm loss function
    learnMSE(windowSize, optimizer) =
        // Window the input signals
        par(n,2+vars.N,window)
        // Calculate the difference between the ground truth and learnable outputs
        // (Is cross necessary?)
        : (routes.cross(2) : - ),pds
        // Calculate loss (this is just for show, since there's no sensitivity threshold)
        : (_ <: loss,_),pds
        // Calculate gradients
        : _,gradients
        // Scale gradients by the learning rate
        : _,optimizer
    with {
        window = ba.slidingMean(windowSize);
        // Loss function (L2 norm)
        loss = ^(2) <: attach(hbargraph("[100]loss",0,.05));
        // A way to move the partial derivatives around.
        pds = si.bus(vars.N);
        // Calculate gradients; for L2 norm: 2 * dy/dx_i * (learnable - groundtruth)
        gradients = _,par(i,vars.N, _,2 : *)
            : routeall
            : par(i,vars.N, * <: attach(hbargraph("[101]gradient %i",-.5,.5)));

        // A utility to duplicate the first input for combination with all remaining inputs.
        routeall = _,si.bus(vars.N)
            : route(vars.N+1,vars.N*2,par(i,vars.N,(1,2*i+1),(i+2,2*(i+1))));
    };

    // Stochastic gradient descent with time-domain mean-squared log error
    learnMSLE(windowSize,optimizer) =
        // Window the input signals
        par(n,2+vars.N,window)
        // Calculate the difference between the ground truth and learnable outputs
        // (Is cross necessary?)
        : _, (_ <: _, _), par(i, vars.N, _)
        : (routes.cross(2) : ((_, 1 : +) : log), ((_, 1 : +) : log) : -),_,pds
        // Calculate loss (this is just for show, since there's no sensitivity threshold)
        : (_ <: loss,_),(_, 1 : +),pds
        // Calculate gradients
        : _,gradients
        // Scale gradients by the learning rate
        : _,optimizer
    with {
        window = ba.slidingMean(windowSize);
        // Loss function: MSLE
        loss = ^(2) <: attach(hbargraph("[100]loss",0,.05));
        // A way to move the partial derivatives around.
        pds = si.bus(vars.N);
        // Calculate gradients: 2 * dy/dx_i * (log(learnable + 1) - log(groundtruth + 1)) / (learnable + 1)
        gradients = (_,_ : /),par(i,vars.N, _,2 : *)
            : routeall
            : par(i,vars.N, * <: attach(hbargraph("[101]gradient %i",-.5,.5)));

        // A utility to duplicate the first input for combination with all remaining inputs.
        routeall = _,si.bus(vars.N)
            : route(vars.N+1,vars.N*2,par(i,vars.N,(1,2*i+1),(i+2,2*(i+1))));
    };

    // Stochastic gradient descent with time-domain Huber loss
    learnHuber(windowSize, optimizer, delta) =
        // Window the input signals
        par(n,2+vars.N,window)
        // Calculate the difference between the ground truth and learnable outputs
        // (Is cross necessary?)
        : (routes.cross(2) : -),pds
        // Calculate loss (this is just for show, since there's no sensitivity threshold)
        : (_ <: loss,_),pds
        // Calculate gradients
        : _,gradients
        // Scale gradients by the learning rate
        : _,optimizer
    with {
        window = ba.slidingMean(windowSize);
        // Loss function: https://en.wikipedia.org/wiki/Huber_loss
        loss = (_ <: (abs : >(delta)), (_ <: (^(2) : /(2)), (delta, (abs, (delta : /(2)) : -) : *))) : select2 <: attach(hbargraph("[100]loss",0,.05));
        // A way to move the partial derivatives around.
        pds = si.bus(vars.N);
        // Calculate gradients; for this error
        gradients = ((_ <: (abs : >(delta)), (_ <: _, ((_ <: (_, abs) : /), delta : *))) : select2),par(i,vars.N, _)
            : routeall
            : par(i,vars.N, * <: attach(hbargraph("[101]gradient %i",-.5,.5)));

        // A utility to duplicate the first input for combination with all remaining inputs.
        routeall = _,si.bus(vars.N)
            : route(vars.N+1,vars.N*2,par(i,vars.N,(1,2*i+1),(i+2,2*(i+1))));
    };

    // MAE loss
    learnMAE(windowSize, optimizer) =
        // Window the input signals
        par(n,2+vars.N,window)
        // Calculate the difference between the ground truth and learnable outputs
        : (routes.cross(2) : - ),pds
        // Calculate loss (this is just for show, since there's no sensitivity threshold)
        : (_ <: loss,_),pds
        // Calculate gradients
        : _,gradients
        // Scale gradients by the learning rate
        : _,optimizer
    with {
        window = ba.slidingMean(windowSize);
        // Loss function (L1 norm)
        loss = abs <: attach(hbargraph("[100]loss",0.,.05));
        // A way to move the partial derivatives around.
        pds = si.bus(vars.N);
        // Calculate gradients; for L1 norm: dy/dx_i * (learnable - groundtruth) / abs(learnable - groundtruth)
        gradients = route(vars.N+1,2*vars.N+1,(1,1),par(i,vars.N,(1,i*2+3),(i+2,2*i+2)))
            : (abs,1e-10 : max),par(i,vars.N, *)
            : route(vars.N+1,vars.N*2,par(i,vars.N,(1,2*i+2),(i+2,2*i+1)))
            : par(i,vars.N, / <: attach(hbargraph("[101]gradient %i",-.5,.5)));
    };

    // As above but takes batches of ground truth input.
    // Needs work...
    learnN(windowSize, optimizer, ntrain) =
        // Window the input signals
        par(n,ntrain+1+vars.N,window)
        // Calculate the difference between each training example and the learned output...
        : route(ntrain+1,2*ntrain,par(n,ntrain,(n+1,2*n+2),(ntrain+1,2*n+1))),si.bus(vars.N)
        : par(n,ntrain,-),si.bus(vars.N)
        // Take the mean of the difference
        : (ba.parallelMean(ntrain) <: attach(hbargraph("mean error",-1,1))),si.bus(vars.N)
        // Calculate loss
        : (_ <: loss,_),pds
        // And gradients
        : _,gradients
        // Scale gradients by the learning rate
        : _,optimizer
    with {
        window = ba.slidingMean(windowSize);
        // Loss function (L1 norm)
        loss = abs <: attach(hbargraph("[100]loss",0,.05));
        // A way to move the partial derivatives around.
        pds = si.bus(vars.N);
        // Calculate gradients; for L1 norm: dy/dx_i * (learnable - groundtruth) / abs(learnable - groundtruth)
        gradients = route(vars.N+1,2*vars.N+1,(1,1),par(i,vars.N,(1,i*2+3),(i+2,2*i+2)))
            : (abs,1e-10 : max),par(i,vars.N, *)
            : route(vars.N+1,vars.N*2,par(i,vars.N,(1,2*i+2),(i+2,2*i+1)))
            : par(i,vars.N, / <: attach(hbargraph("[101]gradient %i",-.5,.5)));
    };

    // Stochastic gradient descent with frequency-domain linear loss function
    learnLinearFreq(windowSize, optimizer) =
        // Window the input signals
        par(n,2+vars.N,window)
        // loss calculation via spectral binning
        : (loss_fn <: _,_), pds
        // Calculate gradients
        : _,gradients
        // Scale gradients by the learning rate
        : _,optimizer
    with {
        window = ba.slidingMean(windowSize);
        // Loss function
        N = 1 << 4;
        loss_fn(truth, learnable) = (truth <: _,an.rfft_analyzer_magsq(N) : smoothing), (learnable <: _,an.rfft_analyzer_magsq(N) : smoothing)
                : _, route(N/2+2, N/2+2, (N/2+2,1),par(i,N/2+1,(i+1,i+2))), par(i,N/2+1,_)
                : !, !, loss_helper;
        // smoothing
        smoothing = _, par(i,N/2+1,si.smooth(ba.tau2pole(0.05)));
        // Helper (this is a bit complex to have in a single fn)
        loss_helper = route(N+2, N+2, par(i,N/2+1,(i+1,x),(i+N/2+2,x+1)
                with{
                    x = 2*i+1;
                }))
                : par(i,N/2+1, -)
                : sum(i,N/2+1,_)
                : /(N/2+1) <: attach(hbargraph("Loss",0,1));
        // A way to move the partial derivatives around.
        pds = si.bus(vars.N);
        // Calculate gradients: loss * dy/dx_i
        gradients =  _,par(i,vars.N, _, 1e-6 : max)
            : routeall
            : par(i,vars.N, * <: attach(hbargraph("[101]gradient %i",-.01,.01)));
        // A utility to duplicate the first input for combination with all remaining inputs.
        routeall = _,si.bus(vars.N)
            : route(vars.N+1,vars.N*2,par(i,vars.N,(1,2*i+1),(i+2,2*(i+1))));
    };
} with {
    unary(primal, tangent) = routing : primal,par(i, vars.N, tangent)
    with {
        routing =
            // Split u N times.
            (_ <: par(i, vars.N + 1, _)),par(i, vars.N, _)
            // Distribute u amongst u': u'1,u, ..., u'N,u
            : _,routes.outerleave(vars.N, 2);
    };

    binary(primal, tangent) = routing : primal,par(i, vars.N, tangent)
    with {
        routing =
            // Duplicate u and v for primal computation.
            // Result: u,u,du1,...,duN, v,v,dv1,...,dvN
            par(j, 2, (_ <: _,_),si.bus(vars.N))
            // Move a copy of v where it can easily be combined with a copy of u.
            // Move u after its partial derivatives.
            // Result: u,v, du1,...duN,u, v,dv1,...,dvN
            : _,routes.swapOuter(vars.N + 2),si.bus(vars.N+1)
            : _,_,(
                // Split u and v. Result: du1,...,duN, u,...,u, v,...,v, dv1,...,dvN
                si.bus(vars.N),(_ <: si.bus(vars.N)),(_ <: si.bus(vars.N)),si.bus(vars.N)
                // Interleave u and v separately. Result: du1,u,...duN,u, v,dv1,...,v,dvN
                : par(i, 2, routes.interleave(vars.N, 2))
                // Interleave u and v together. Result: du1,v,u,dv1,...,duN,v,u,dvN
                : routes.interleave(2*vars.N, 2)
            );
    };

    // Avoid division by zero.
    // This uses a lambda abstraction and isn't differentiable.
    safediv(x, y) = x,(y,(ma.EPSILON,-1*ma.EPSILON : select2(y < 0)) : select2(abs(y) < ma.EPSILON)) : /;

    // This one works for guaranteed-positive denominators.
    positiveSafediv = _,_,ma.EPSILON : _,max : /;
};

//===========================Activation Functions===========================
//
//==========================================================================
// We define activation functions in an environment.
activations = environment {
    sigmoid(d) = d.diff(0),d.diff(_) : d.diff(-) : d.diff(1),d.diff(exp),d.diff(1) : d.diff(_),d.diff(+) : d.diff(/);
};

//===========================Loss Functions=================================
//
//==========================================================================
// Similar to activation functions, we define loss functions in an environment.
// This is especially effective when we need to use the loss functions in a neural network.
// Note that these loss functions can ONLY be used in the context of a neural network and only the final fully connected layer.
// This is because the loss functions are defined in terms of the output of the neural network.
losses = environment {
    L1(windowSize, y, N) = lossFn(windowSize, y, _, N)
    with {
        lossFn(windowSize, y, yHat, N) = yHat - y, par(i, 2*N+1, _)
                                    : window(windowSize, N)
                                    : d.diff(abs)
        with {
            window(size, N) = par(i, 2*N+2, ba.slidingMean(size));
            v = vars((par(i, 2*N+1, _)));
            d = env(v);
        };
    };

    L2(windowSize, y, N) = lossFn(windowSize, y, _, N)
    with {
        lossFn(windowSize, y, yHat, N) = yHat - y, par(i, 2*N+1, _)
                                    : window(windowSize, N), d.diff(2)
                                    : d.diff(^)
        with {
            window(size, N) = par(i, 2*N+2, ba.slidingMean(size));
            v = vars((par(i, 2*N+1, _)));
            d = env(v);
        };
    };
};

//========================Neuron network blocks=============================
//
//==========================================================================
// Core weights and biases definitions -- this is the core of the neural network.
weights(n, learningRate) = par(i, n, *(learningRate) : -~_ <: attach(hbargraph("weight%i", -1, 1)));
bias(learningRate) = *(learningRate) : -~_ <: attach(hbargraph("bias", -1, 1));

// Here, we define the neuron. This is the smallest element of a fully connected layer.
// It takes in the number of inputs, the activation function, and the learning rate.
// Experimentally, we noticed that learning rate should be of the magnitude 1e-5 or less; otherwise, the system seems unstable.
// Input : w, b, x
// Subprocesses: Convert inputs to z = w*x + b, apply activation function, calculate gradients
// Output: y, dy/dw1, dy/dw2... dy/dwN, dy/db, dy/dx1, dy/dx2... dy/dxN
neuron(N, activation, learningRate) = cross : update_params : act : grad_route(N)
    with {
        // Appropriately routing the signals
        cross = routes.cross(2*N+1) : routes.cross(N), _, routes.cross(N)
            : par(i, N, _), route(N+1, N+1, par(i, N, (i+2, i+1)), (1,N+1));

        // Recursively updating the weights and biases
        update_params = par(i, N, _), weights(N, learningRate), bias(learningRate);

        // Routing the signals to the activation function
        routing = route(2*N+1, 2*N+1,
            par(i, N, (i+1,2*i+1), (i+1+N,2*i+2)), (2*N+1,2*N+1));

        // Applying the activation function
        act = routing : (par(i, N, v.var(i+1),v.var(i+1+N) : d.diff(*)) : d.sumall(N)),v.var(2*N+1) : d.diff(+)
            : acti
            with {
                v = vars((par(i, 2*N+1, _)));
                d = env(v);
                acti = activation(d);
            };

        // Reset the routes -- making the signals ready for the gradient calculation
        grad_route(N) = _,routes.cross(2*N),_ : _,routes.cross(N),routes.cross(N),_
                    : _,par(i, N, _),route(N+1, N+1, (N+1, 1), par(i, N, (i+1, i+2)));
    };

// Fully connected (FC) layer is one of the most popular simplified layers of a neural network. It consists of a collection of neurons.
// It takes in the number of neurons, the number of inputs, the activation function, and the learning rate.
// Input : N (number of neurons), n (number of inputs), activation function, learning rate
// Subprocesses: Routing the signals to the neurons
// Output: y1, y2, ... yN, dy1/dw1, dy1/dw2... dy1/dwN, dy1/db1, dy2/dw1, dy2/dw2... dy2/dwN, dy2/db2, ... dyN/dw1, dyN/dw2... dyN/dwN, dyN/dbN, dy1/dx1, dy1/dx2... dy1/dxN, dy2/dx1, dy2/dx2... dy2/dxN, dyN/dx1, dyN/dx2... dyN/dxN
fc(N, n, activationFn, learningRate) = // Route signals appropriately -- allow the inputs and corresponding weights to be sent to each neuron
                                    route(N*(n+1)+n, N*(2*n+1),
                                    par(i, (n+1)*N, (i+1,i+1+n*int(i / (n+1)))),
                                    par(i, N, par(j, n, (j+1+(n+1)*N, (j+1+(i+1)*(n+1)+i*(n))))))
                                    // Apply the neuron to each set of inputs
                                    : par(i, N, (neuron(n, activationFn, learningRate)))
                                    // Pushing all the outputs (y) to the top and gradients and biases to the bottom
                                    : route((2*n+2)*N, (2*n+2)*N, par(i, N, (i*(2*n+2)+1, i+1)),
                                    par(i, N, par(j, 2*n+1, (i*(2*n+2)+j+2, N+i*(2*n+1)+j+1))));

// Backpropagation is implemented as a separate differentiable environment.
// Note that this implementation has a limitation that violates automatic differentiation. We follow symbolic differentiation in certain stages of the process.
// Read the documentation for more details.
backpropNN(params) = environment {
        // Calculate the number of FCs in the network
        N = ba.count(params) / 2;

        // 0 = N-indexed (from 0 to N-1)
        // This recursive function calculates the number of gradients of the previous layers that should be allowed to pass through.
        // This is necessary for the routing process. Note that this recursive function is N-indexed i.e Nth layer is represented as 0 here.
        // This means that at the Nth layer, the number of gradients that should be allowed to pass through is 0.
        // This is because the Nth layer is the last layer in the network.
        next_signals(0) = 0;
        next_signals(n) = next_signals(n-1) + ((ba.take(2*(N-n+1), params))*2+1)*(ba.take(2*(N-n+1)-1, params));

        // 1 = 1-indexed (from 1 to N-1)
        // This recursive function calculates the number of gradients of the next layers that should be allowed to pass through.
        // This is necessary for the routing process. Note that this recursive function is 1-indexed. Note that 0 is not valid here due to the fact that the output neuron already has gradients ready for backpropagation.
        // This means that at the 1st layer, the number of gradients that should be allowed to pass through is number of gradients of the output layer.
        // This is because the 1st layer is the first layer in the network. Furthermore, 1-indexing is used to allow for the 0th layer to be the output layer.
        prev_signals(1) = (ba.take(2, params) + 1)*ba.take(1, params) + 1;
        prev_signals(n) = prev_signals(n-1) + ((ba.take(2*n, params))+1)*(ba.take(2*n-1, params));

        // This consolidates the previous, backpropagation of the current layer and next gradients in one function.
        backprop_helper(M) = par(i, prev_signals(M), _), df.backpropFC(ba.take(2*M+2, params), ba.take(2*M+1, params)), par(i, next_signals(N-M-1), _);

        // This recursive function allows us to start the backpropagation process for entire NN.
        start(1) = backprop_helper(1);
        start(N) = start(N-1) : backprop_helper(N);
};

// This is the core of the backpropagation process. This is where we calculate the gradients per FC.
// Note that this process follows symbolic differentiation. Read the documentation about the reasoning for the same.
backpropFC(N, n) = // Start off with routing the outputs of the previous backpropagation layer to the chain rule mechanism.
        route((2*N+1)*n + n, (2*N+1)*n + n, par(i, n, (i+1, i+1+(2*N+1)*i),
        par(j, 2*N+1, (n+j+1+(2*N+1)*i, (i*(2*N+2)+1) + j+1))))
        // Apply the chain rule mechanism to calculate the gradients of the current FC.
        : par(i, n, autoChainRule) : prepareGrads(N, n)
        // Average the gradients of the current FC -- this is necessary for the backpropagation process.
        : par(i, (N+1)*n, _), gradAveraging(N, n)
        with {
            // This is the chain rule mechanism. It is used to calculate the gradients of the current FC.
            autoChainRule = route(2*N+2, 4*N+2, par(i, 2*N+1, (1, 2*i+1), (i+2, 2*i+2))) : autoChain;
            // This step completes the chain rule mechanism and provides the appropriate gradients for recursion.
            autoChain = par(i, 2*N+1, (v.var(1), d.input : d.diff(*)) : _, si.block(4*N+2)
                        with {
                            v = df.vars((par(j, 4*N+2, _)));
                            d = df.env(v);
                        }
                    );

            // Prepares the gradients to be recursed backwards to the FC for updating the weights and biases.
            prepareGrads(N, n) = route((2*N+1)*n, (2*N+1)*n, par(i, n,
                        par(j, N+1, (j+1+i*(2*N+1), (1+N)*i+(j+1))),
                        par(k, N, (k+1+N+1+(i)*(2*N+1),(k+1+i*N+((N+1)*n))))));

            // This averages the gradients of the current FC. This is necessary for the backpropagation process.
            gradAveraging(N, n) = route(N*n, N*n, par(i, N+1, par(j, n, (j+i*N+1, i+1+j*n)))) : par(i, N, sum(j, n, _)) : par(i, N, /(n));
        };

// More utilities...
routes = environment {
    // Differentiable analog to ro.interleave
    // interleave(2, 2) Maps input signals 1,2,3,4 to 1,3,2,4
    //
    // twotwo = _,_,_,_ <: _,!, _,!, !,_, !,_; // ro.interleave(2,2)
    // twothree = _,_,_,_,_,_ <: _,!, _,!, _,!, !,_, !,_, !,_; // ro.interleave(2,3)
    // threetwo = _,_,_,_,_,_ <: _,!,!, _,!,!, !,_,!, !,_,!, !,!,_, !,!,_; // ro.interleave(3,2)
    // fourtwo = _,_,_,_,_,_,_,_ <: _,!,!,!, _,!,!,!, !,_,!,!, !,_,!,!, !,!,_,!, !,!,_,!, !,!,!,_, !,!,!,_; // 4,2
    //
    // In general, R*C groups of R signals.
    // The rth signal in each of C subgroups passes; the rest are cut.
    interleave(1, C) = par(i, C, _);
    interleave(R, C) = par(i, R * C, _) <: par(r, R, par(c, C, distribute(r, 1, R - r - 1)));

    // outerleave(2, 2) Maps input signals 1,2,3,4 to 3,1,4,2
    // R*C groups of R*C-(R-1) signals, in R subgroups.
    outerleave(R, C) = par(i, R * C, _) <:
        par(r, R, par(c, C, distribute((R - r) * (C - 1) - c, 1, c + r * (C - 1))));

    // Differentiable cross
    cross(0) = distribute(0, 0, 0);
    cross(n) = par(i, n, _) <: par(i, n, distribute(n - i - 1, 1, i));

    // Differentiable crossNM
    crossNM(n, m) = par(i, n + m, _) <: distribute(n, n + m, m);

    // Swap the first and nth of n signals. Equivalent to `route(n, n, 1, n, 2, 2, ..., n, 1)`
    swapOuter(0) = distribute(0, 0, 0);
    swapOuter(1) = distribute(0, 1, 0);
    swapOuter(2) = cross(2);
    swapOuter(n) = par(i, n, _) <: distribute(n - 1, 1, 0),par(i, n-2, distribute(i + 1, 1, n - 2 - i)),distribute(0, 1, n - 1);

    // Duplicate some signals and move the duplicates to the bottom.
    splitCrossDown(n, nSplit) = (par(i, nSplit, _ <: _,_) : interleave(2, nSplit)),par(i, n - nSplit, _)
        : crossNM(nSplit, n);
} with {
    // Helper function for distributing groups of signals.
    // p: number of signals to cut at the start of the group.
    // q: number of signals to pass.
    // r: number of signals to cut at the end of the group.
    distribute(0, q, 0) = par(i, q, _);
    distribute(p, q, 0) = par(i, p, !),par(j, q, _);
    distribute(0, q, r) = par(i, q, _),par(j, r, !);
    distribute(p, q, r) = par(i, p, !),par(j, q, _),par(k, r, !);
};
